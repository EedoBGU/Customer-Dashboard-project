{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix customer dashboard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import time\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data from redshift to dataframe named \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Enter Here the table of the new merchant data instead of playground.lightricks_data\n",
    "\n",
    "QUERY = \"\"\"\n",
    "SELECT *\n",
    "FROM playground.chegg_acquiring \n",
    "\"\"\"\n",
    "\n",
    "def extract_data_from_result(result):\n",
    "    columns = []\n",
    "    rows = []\n",
    "\n",
    "    for column in result[\"ColumnMetadata\"]:\n",
    "        columns.append(column[\"name\"])\n",
    "\n",
    "    for result_row in result[\"Records\"]:\n",
    "        row_values = []\n",
    "        for result_column in result_row:\n",
    "            # Handle None values\n",
    "            row_values.append(list(result_column.values())[0] if result_column else None)\n",
    "        rows.append(row_values)\n",
    "\n",
    "    return columns, rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    redshift_client = boto3.client(\"redshift-data\", region_name=\"us-east-1\")\n",
    "    \n",
    "    # Execute the query\n",
    "    response = redshift_client.execute_statement(\n",
    "        Database=\"analytics\",\n",
    "        DbUser='eedobounce',\n",
    "        ClusterIdentifier=\"analytics-redshift\",\n",
    "        Sql=QUERY,\n",
    "        \n",
    "    )\n",
    "    print(f\"Query Response: {response}\")\n",
    "    \n",
    "    query_id = response[\"Id\"]\n",
    "    query_status = \"STARTED\"\n",
    "    \n",
    "    # Check query status in a loop\n",
    "    while query_status in [\"STARTED\", \"SUBMITTED\", \"PICKED\", \"WIP\"]:\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "        query_state = redshift_client.describe_statement(Id=query_id)\n",
    "        query_status = query_state[\"Status\"]\n",
    "        print(f\"Query State: {query_state}\")\n",
    "    \n",
    "    # Check the final status of the query\n",
    "    if query_status == \"FINISHED\":\n",
    "        print(\"Query finished successfully.\")\n",
    "        try:\n",
    "            results = redshift_client.get_statement_result(Id=query_id)\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Resource not found.\")\n",
    "            results = {\"ColumnMetadata\": [], \"Records\": []}\n",
    "        \n",
    "        # Extract data and create DataFrame\n",
    "        columns, rows = extract_data_from_result(result=results)\n",
    "        data = pd.DataFrame(rows, columns=columns)\n",
    "        print(\"DataFrame created successfully.\")\n",
    "        print(data.head())\n",
    "    else:\n",
    "        print(f\"Query did not finish successfully. Final status: {query_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download exchange rate table from Redshift \n",
    "QUERY = \"\"\"\n",
    "SELECT *\n",
    "FROM dms_analyst.exchangerate\n",
    "\"\"\"\n",
    "\n",
    "def extract_data_from_result(result):\n",
    "    columns = []\n",
    "    rows = []\n",
    "\n",
    "    for column in result[\"ColumnMetadata\"]:\n",
    "        columns.append(column[\"name\"])\n",
    "\n",
    "    for result_row in result[\"Records\"]:\n",
    "        row_values = []\n",
    "        for result_column in result_row:\n",
    "            # Handle None values\n",
    "            row_values.append(list(result_column.values())[0] if result_column else None)\n",
    "        rows.append(row_values)\n",
    "\n",
    "    return columns, rows\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    redshift_client = boto3.client(\"redshift-data\", region_name=\"us-east-1\")\n",
    "    \n",
    "    # Execute the query to fetch the exchange rate table\n",
    "    response = redshift_client.execute_statement(\n",
    "        Database=\"analytics\",\n",
    "        DbUser='eedobounce',\n",
    "        ClusterIdentifier=\"analytics-redshift\",\n",
    "        Sql=QUERY,\n",
    "    )\n",
    "    print(f\"Query Response: {response}\")\n",
    "    \n",
    "    query_id = response[\"Id\"]\n",
    "    query_status = \"STARTED\"\n",
    "    \n",
    "    # Check query status in a loop\n",
    "    while query_status in [\"STARTED\", \"SUBMITTED\", \"PICKED\", \"WIP\"]:\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking the status again\n",
    "        query_state = redshift_client.describe_statement(Id=query_id)\n",
    "        query_status = query_state[\"Status\"]\n",
    "        print(f\"Query State: {query_state}\")\n",
    "    \n",
    "    # Check the final status of the query\n",
    "    if query_status == \"FINISHED\":\n",
    "        print(\"Query finished successfully.\")\n",
    "        try:\n",
    "            results = redshift_client.get_statement_result(Id=query_id)\n",
    "        except redshift_client.exceptions.ResourceNotFoundException:\n",
    "            print(\"Resource not found.\")\n",
    "            results = {\"ColumnMetadata\": [], \"Records\": []}\n",
    "        \n",
    "        # Extract data and create DataFrame for the exchange rate table\n",
    "        columns, rows = extract_data_from_result(result=results)\n",
    "        exchange_rate_table = pd.DataFrame(rows, columns=columns)\n",
    "        print(\"Exchange Rate DataFrame created successfully.\")\n",
    "        print(exchange_rate_table.head())\n",
    "    else:\n",
    "        print(f\"Query did not finish successfully. Final status: {query_status}\")\n",
    "\n",
    "\n",
    "exchange_rate_table['date'] = pd.to_datetime(exchange_rate_table['date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, columns_mapping, unique_columns, sort_data):\n",
    "    \n",
    "    # Mapping of columns\n",
    "    \n",
    "    for k, v in columns_mapping.items():\n",
    "        if v in data.columns:\n",
    "            data.rename(columns={v: k}, inplace=True)\n",
    "    \n",
    "    #Remove duplicates based on unique_columns and duplicate_strategy\n",
    "    sorted_by = unique_columns + sort_data\n",
    "    data = data.sort_values(by=sorted_by)\n",
    "    data.drop_duplicates(subset=unique_columns, keep='first', inplace=True)\n",
    "    \n",
    "    # Status mapping Left is the status in the data and right is the status we want to map to\n",
    "    status_mapping = {\n",
    "        'Approved': 'Approved',\n",
    "        'Declined': 'Declined',\n",
    "        'Failed': 'Declined',  \n",
    "        'Paid': 'Approved',\n",
    "        'Success': 'Approved'\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    \n",
    "    #status mapping    \n",
    "    data['status'] = data['status'].apply(lambda x: status_mapping.get(x, x))\n",
    "\n",
    "    # Declined reason mapping\n",
    "    # Define conditions and corresponding values as a dictionary\n",
    "    regex_values_dict = {\n",
    "        r'.*insufficient.*': 'insufficient funds',   \n",
    "        r'payment.*complete': 'payment complete',\n",
    "        r'do.*not.*honor': 'do not honor',\n",
    "        r'transaction.*not.*allowed': 'transaction not allowed',\n",
    "        r'stripe.*block': 'blocked by Stripe',\n",
    "        r'.*did.*not.*return': 'no further details',  \n",
    "        r'block.*lists': 'block lists rules',\n",
    "        r'.*expired.*card': 'expired card',\n",
    "        r'.*try.*again.*later': 'try again later',\n",
    "        r'.*invalid.*account': 'invalid account',\n",
    "        r'.*invalid.*pin.*': 'invalid pin',\n",
    "        r'.*invalid.*cvc.*': 'invalid cvc',\n",
    "        r'.*invalid.*amount.*': 'invalid amount',\n",
    "        r'incorrect.*number': 'incorrect number',\n",
    "        r'.*incorrect.*cvc.*': 'incorrect cvc',\n",
    "        r'.*blocked.*by.*merchant.*rule': 'blocked by merchant rule',\n",
    "        r'pickup.*card': 'pickup_card',\n",
    "        r'processing.*error': 'processing error',\n",
    "        r'stolen.*card': 'stolen card',\n",
    "        r'lost.*card': 'lost card',\n",
    "        r'.*fraud.*': 'Processor Declined - Fraud Suspected',\n",
    "        r'limit.*exceeded': 'insufficient funds',\n",
    "        r'approved': 'Approved',\n",
    "        r'reenter.*transaction': 'reenter_transaction',\n",
    "        r'Life cycle.*': 'cannot authorize at this time life cycle',\n",
    "        r'82.*Policy': 'expired card',\n",
    "        r'.*Insufficient.*': 'insufficient funds',\n",
    "        r'card_velocity_exceeded': 'card velocity exceeded',\n",
    "        r'withdrawal_count_limit_exceeded': 'withdrawal count limit exceeded',\n",
    "        r'incorrect_number': 'incorrect number',\n",
    "        r'do_not_honor': 'do not honor',\n",
    "        r'call_issuer': 'call issuer',\n",
    "        r'transaction_not_allowed': 'transaction not allowed',\n",
    "        r'revocation_of_authorization': 'revocation of authorization',\n",
    "        r'revocation_of_all_authorization': 'revocation of all authorization',\n",
    "        r'reenter_transaction': 'reenter transaction',\n",
    "        r'service_not_allowed': 'service not allowed',\n",
    "        r'generic_decline': 'generic decline',\n",
    "        r'no_action_taken': 'no action taken',\n",
    "        r'stop_payment_order': 'stop payment order',\n",
    "        r'try_again_later': 'try again later',\n",
    "        r'processing_error': 'processing error',\n",
    "        r'do_not_try_again': 'do not try again',\n",
    "        r'issuer_not_available': 'issuer not available',\n",
    "        r'.*invalid_number.*': 'invalid number',\n",
    "        r'highest_risk_level': 'highest risk level',\n",
    "        r'NULL': 'stripe blocked payment',\n",
    "        r'previously_declined_do_not_retry': 'stripe blocked payment',\n",
    "        r'requested_block_on_incorrect_cvc': 'requested block on incorrect cvc',\n",
    "        r'.*expired_card.*': 'expired card',\n",
    "        r'invalid_expiry_month': 'invalid expiry month',\n",
    "        r'invalid_expiry_year': 'invalid expiry year',\n",
    "        r'.*stolen.*card.*': 'stolen card',\n",
    "        r'.*lost_card.*': 'lost card',\n",
    "        r'.*pickup_card.*': 'pickup card',\n",
    "        r'blocklist': 'blocklist',\n",
    "        r'merchant_blacklist': 'merchant blacklist',\n",
    "        r'rule': 'merchant rule',\n",
    "        r'.*incorrect_pin.*': 'incorrect pin',\n",
    "        r'pin_try_exceeded': 'pin try exceeded',\n",
    "        r'authentication_required': 'authentication required',\n",
    "        r'approve_with_id': 'approve with id',\n",
    "        r'security_violation': 'security violation',\n",
    "        r'elevated_risk_level': 'elevated risk level',\n",
    "        r'restricted_card': 'card restriction',\n",
    "        r'fraudulent': 'stripe fraud',\n",
    "        r'currency_not_supported': 'currency not supported',\n",
    "        r'card_not_supported': 'card not supported',\n",
    "        r'duplicate_transaction': 'duplicate transaction',\n",
    "        r'incorrect_zip': 'incorrect zip',\n",
    "        r'new_account_information_available': 'new account information available',\n",
    "        r'not_permitted': 'not permitted',\n",
    "        r'testmode_decline': 'stripe test decline',\n",
    "        r'offline_pin_required': 'offline pin required',\n",
    "        r'online_or_offline_pin_required': 'online or offline pin required',\n",
    "        r'1000': 'approved',\n",
    "        r'1001': 'approved',\n",
    "        r'1002': 'approved',\n",
    "        r'1003': 'approved',\n",
    "        r'1004': 'approved',\n",
    "        r'2001': 'insufficient funds',\n",
    "        r'2002': 'limit exceeded',\n",
    "        r'2003': 'incorrect number',\n",
    "        r'2005': 'incorrect number',\n",
    "        r'2000': 'do not honor',\n",
    "        r'2044': 'declined call issuer',\n",
    "        r'2046': 'declined',\n",
    "        r'2013': 'stolen card',\n",
    "        r'2007': 'invalid account',\n",
    "        r'2004': 'expired card',\n",
    "        r'2047': 'pickup card',\n",
    "        r'2010': 'incorrect cvc',\n",
    "        r'2021': 'security violation',\n",
    "        r'2020': 'violation',\n",
    "        r'2057': 'card restriction',\n",
    "        r'3000': 'try again later',\n",
    "        r'2026': 'invalid merchant id',\n",
    "        r'2014': 'processor declined fraud suspected',\n",
    "        r'2019': 'invalid transaction',\n",
    "        r'2048': 'invalid amount',\n",
    "        r'2009': 'no such issuer',\n",
    "        r'2016': 'duplicate transaction',\n",
    "        r'2037': 'already reversed',\n",
    "        r'2017': 'cardholder stopped billing',\n",
    "        r'2018': 'cardholder stopped billing',\n",
    "        r'2056': 'transaction amount exceeded division amount',\n",
    "        r'2041': 'call issuer',\n",
    "        r'2038': 'processor declined',\n",
    "        r'2015': 'transaction not allowed',\n",
    "        r'2034': 'no action taken',\n",
    "        r'2043': 'do not try again',\n",
    "        r'2008': 'card account length error',\n",
    "        r'2022': 'updated cardholder available',\n",
    "        r'2006': 'invalid expiration date',\n",
    "        r'2053': 'lost or stolen',\n",
    "        r'2012': 'lost card',\n",
    "        r'2102': 'incorrect pin',\n",
    "        r'2103': 'pin try exceeded',\n",
    "        r'2105': 'cannot authorize at this time life cycle',\n",
    "        r'2106': 'cannot authorize at this time policy',\n",
    "        r'2107': 'card not activated',\n",
    "        r'.*Deny.*': 'declined',\n",
    "        r'.*Payment complete.*': 'Approved',\n",
    "        r'.*85 : No reason to decline a request for account number verification.*': 'Approved'\n",
    "    }\n",
    "\n",
    "    #Can do a Lamda function instead\n",
    "    # Use regular expressions to apply conditions and assign values to the new column based on the dictionary\n",
    "    for regex, value in regex_values_dict.items():\n",
    "        data.loc[data['decline_reason'].str.contains(regex, case=False, na=False, regex=True), 'decline_reason'] = value\n",
    "\n",
    "    # Loop over unique decline reasons\n",
    "    for reason in data['decline_reason'].unique():\n",
    "        # Check if the reason is not found in the values dictionary\n",
    "        if reason not in regex_values_dict.values():\n",
    "            # Assign 'other' to rows where the decline reason matches\n",
    "            data.loc[data['decline_reason'] == reason, 'decline_reason'] = 'other'\n",
    "    \n",
    "    \n",
    "    # Ensure the 'currency' column is of type string and convert to uppercase\n",
    "    data['currency'] = data['currency'].astype(str).str.upper()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_validation(data, unique_columns):\n",
    " \n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.expand_frame_repr', False)\n",
    "    \n",
    "    # Check for duplicate transaction IDs\n",
    "    duplicate_transactions = data.duplicated(subset= unique_columns).sum()\n",
    "    print(f\"Duplicate transaction by unique columns: {duplicate_transactions}\")\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    status_counts = data['status'].value_counts(normalize=True)\n",
    "    approved_count = status_counts.get('Approved', 0)\n",
    "    declined_count = status_counts.get('Declined', 0)\n",
    "\n",
    "    # Check if the percentage of \"Approved\" or \"Declined\" is less than 10%\n",
    "    print( \"Check the percentage of 'Approved' or 'Declined' \")\n",
    "    if approved_count < 0.1:\n",
    "        print(\"######### Warning: Approved transactions are less than 10% of the total data.###############\")\n",
    "    if declined_count < 0.1:\n",
    "        print(\"######### Warning: Declined transactions are less than 10% of the total data.###############\")\n",
    "\n",
    "    print(data['status'].value_counts())\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Check for 'created' in column names and if it's a timestamp\n",
    "    print(\"Check timestamp if exists in the data:\")\n",
    "    try:\n",
    "        # Attempt to convert to datetime\n",
    "        data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "    except ValueError as e:\n",
    "        print(f\"Created at is now a possible time_stamp: {e}\")    \n",
    "    if pd.api.types.is_datetime64_any_dtype(data['created_at']):\n",
    "        min_date = data['created_at'].min()\n",
    "        max_date = data['created_at'].max()\n",
    "        print(\"created_at is a timestamp column.\")\n",
    "        print(\"Minimum date:\", min_date)\n",
    "        print(\"Maximum date:\", max_date)\n",
    "        print(\"Date difference (days):\", (max_date - min_date).days)\n",
    "    else:\n",
    "        print(\"No timestamp column containing 'created' found.\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Checks for US currency:\")\n",
    "    currency_counts = data['currency'].value_counts(normalize=True) \n",
    "    usd_counts = currency_counts.get('USD', 0)\n",
    "    if usd_counts < 0.1:\n",
    "        print(\"Warning: Missing data from US\")\n",
    "    else:\n",
    "        print(\"US data is exist.\")\n",
    "    print(data['currency'].value_counts(normalize=True))\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    # Check which currencies are missing in the exchange rate table\n",
    "    exchange_currencies = exchange_rate_table['currency'].unique()\n",
    "    data_currencies = data['currency'].unique()\n",
    "    missing_currencies = [currency for currency in data_currencies if currency not in exchange_currencies]\n",
    "    print(\"Missing Currencies from table:\")\n",
    "    print(missing_currencies)\n",
    "    \n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns') \n",
    "    pd.reset_option('display.expand_frame_repr', False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data, grouped_by, order_by, fixed_exchange_rates):\n",
    "    #Adding root transactions\n",
    "    if order_by = 'status'\n",
    "        status_priority = {'Declined': 1, 'Approved': 2}\n",
    "\n",
    "        # Apply custom sorting within each group and assign rank\n",
    "        data['retry_number'] = (\n",
    "            data.assign(status_priority=data['status'].map(status_priority))\n",
    "            .sort_values(by=grouped_by + ['status_priority', order_by])\n",
    "            .groupby(grouped_by)\n",
    "            .cumcount() + 1\n",
    "        )\n",
    "\n",
    "    # Drop the helper column used for sorting\n",
    "    data.drop(columns='status_priority', inplace=True)\n",
    "    else:\n",
    "        data['retry_number'] = data.groupby(grouped_by)[order_by].rank(method='first')\n",
    "    \n",
    "    # Adding  last retry decision \n",
    "    grouped_data = data.groupby(grouped_by)['status'].apply(lambda x: x.isin(['Success', 'Approved']).max()).reset_index(name='last_retry_decision')\n",
    "\n",
    "    # Merge the aggregated result back to the original DataFrame based on the grouping columns\n",
    "    data = data.merge(grouped_data, on=grouped_by, how='left')\n",
    "    \n",
    "    #last retry decline reason for renewals\n",
    "    # Determine the max retry number for each transaction_id\n",
    "    max_retry_per_transaction = data.groupby(grouped_by)['retry_number'].max().reset_index()\n",
    "\n",
    "    # Merge with original dataframe to get corresponding decline reason for max retry\n",
    "    merged_data = pd.merge(data, max_retry_per_transaction, on=grouped_by + ['retry_number'], how='inner')\n",
    "    # data['last_retry_decline_reason'] = merged_data['decline_reason']\n",
    "    # Create a dictionary mapping transaction_id to the corresponding decline_reason for the max retry number\n",
    "    transaction_id_to_decline_reason = dict(zip(merged_data['transaction_id'], merged_data['decline_reason']))\n",
    "\n",
    "    # Map the transaction_id in data to the corresponding decline_reason using the dictionary\n",
    "    data['last_retry_decline_reason'] = data['transaction_id'].map(transaction_id_to_decline_reason)\n",
    "    \n",
    "    # Add USD amount column\n",
    "    data = pd.merge(data, exchange_rate_table, how='left', left_on=['currency', 'created_at'], right_on=['currency', 'date'])\n",
    "\n",
    "    # Apply lambda function to calculate usd_amount\n",
    "    data['usd_amount'] = data.apply(\n",
    "        lambda row: round(row['amount'] * (row['rate'] if pd.notna(row['rate']) else fixed_exchange_rates.get(row['currency'], None)), 1) if fixed_exchange_rates.get(row['currency'], None) or pd.notna(row['rate']) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    columns_to_remove = ['id', 'rate', 'date', 'created_at_y', 'updated_at']\n",
    "    data.drop(columns=columns_to_remove, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the data with the instruction below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mapping = {\n",
    "    'transaction_id': 'transaction_id',\n",
    "    'decline_reason': 'processor_response_text',\n",
    "    'created_at': 'invoice_date',\n",
    "    'amount': 'amount',\n",
    "    'currency': 'iso_currency_code',\n",
    "    'buyer_id': 'user_id',  # - Customer email/ subscription_id (For identifying self retries)\n",
    "    'retry': '', #If they have column fo retry \n",
    "    'status': 'status', # Success/Failed/Approved/Declined etc..\n",
    "    'type': '', # Transaction type checkout/renewal\n",
    "    'last_retry_decision': '', # - Decision of the last retry (Approve/Decline) will add after\n",
    "    'usd_amount': '', # - Will add after\n",
    "    'retry_number': '', # - Will add after\n",
    "    'last_retry_decline_reason': '' # if it's a renewal will add after\n",
    "}\n",
    "\n",
    "\n",
    "#Here write the columns needs to be unique with no duplicates\n",
    "unique_columns = ['transaction_id', 'created_at', 'amount']  # Enter column name who should be unique\n",
    "sort_data = []\n",
    "data_clean = clean_data(data, columns_mapping, unique_columns, sort_data) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate transaction by unique columns: 0\n",
      "\n",
      "\n",
      "Check the percentage of 'Approved' or 'Declined' \n",
      "status\n",
      "Approved     474403\n",
      "Declined     267987\n",
      "Cancelled       534\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Check timestamp if exists in the data:\n",
      "created_at is a timestamp column.\n",
      "Minimum date: 2024-03-01 00:00:00\n",
      "Maximum date: 2024-03-31 00:00:00\n",
      "Date difference (days): 30\n",
      "\n",
      "\n",
      "Checks for US currency:\n",
      "US data is exist.\n",
      "currency\n",
      "USD    0.849422\n",
      "CAD    0.038443\n",
      "AUD    0.024023\n",
      "KRW    0.018112\n",
      "GBP    0.016630\n",
      "TRY    0.011495\n",
      "PHP    0.009230\n",
      "ZAR    0.008471\n",
      "HKD    0.005614\n",
      "SAR    0.005403\n",
      "MXN    0.004174\n",
      "SGD    0.002806\n",
      "AED    0.002380\n",
      "MYR    0.002240\n",
      "IDR    0.001470\n",
      "INR    0.000052\n",
      "EUR    0.000035\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "\n",
      "Missing Currencies from table:\n",
      "['USD', 'MYR', 'HKD', 'SAR', 'AED', 'IDR', 'PHP', 'TRY', 'SGD', 'ZAR', 'MXN']\n"
     ]
    }
   ],
   "source": [
    "data_validation(data_clean, unique_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to group by and order by in order to determine retry number\n",
    "grouped_by = ['user_id', 'created_at', 'amount'] \n",
    "\n",
    "order_by = 'status'\n",
    "fixed_exchange_rates = {\n",
    "    'USD': 1,\n",
    "    'BGN': 0.56,\n",
    "    'DKK': 0.15,\n",
    "    'NZD': 0.63,\n",
    "    'RSD': 0.0092,\n",
    "    'PLN': 0.25,\n",
    "    'BDT': 0.0094,\n",
    "    'SGD': 0.76,\n",
    "    'TRY': 0.034,\n",
    "    'KES': 0.0064,\n",
    "    'NOK': 0.098,\n",
    "    'ZAR': 0.055,\n",
    "    'PEN': 0.27,\n",
    "    'ISK': 0.0074,\n",
    "    'AMD': 0.0025,\n",
    "    'DZD': 0.0074,\n",
    "    'USD': 1,\n",
    "    'AUD': 0.68,\n",
    "    'HUF': 0.0029,\n",
    "    'DOP': 0.017,\n",
    "    'BRL': 0.21,\n",
    "    'MXN': 0.059,\n",
    "    'CHF': 1.19,\n",
    "    'UAH': 0.026,\n",
    "    'RUB': 0.011,\n",
    "    'AED': 0.27,\n",
    "    'CRC': 0.0019,\n",
    "    'THB': 0.029,\n",
    "    'EUR': 1.10,\n",
    "    'MYR': 0.22,\n",
    "    'COP': 0.00026,\n",
    "    'VND': 0.000041,\n",
    "    'GBP': 1.27,\n",
    "    'ARS': 0.0012,\n",
    "    'GEL': 0.37,\n",
    "    'PKR': 0.0036,\n",
    "    'SAR': 0.27,\n",
    "    'ILS': 0.28,\n",
    "    'INR': 0.012,\n",
    "    'HKD': 0.13,\n",
    "    'SEK': 0.099,\n",
    "    'JMD': 0.0065,\n",
    "    'PHP': 0.018,\n",
    "    'CLP': 0.0011,\n",
    "    'JPY': 0.0071,\n",
    "    'EGP': 0.032,\n",
    "    'KRW': 0.00077,\n",
    "    'RON': 0.22,\n",
    "    'TWD': 0.032,\n",
    "    'TND': 0.32,\n",
    "    'NGN': 0.0011,\n",
    "    'IDR': 0.000064,\n",
    "    'CAD': 0.76,\n",
    "    'PHP': 0.017\n",
    "    \n",
    "}\n",
    "fix_data =  feature_engineering(data_clean, grouped_by, order_by, fixed_exchange_rates)\n",
    "fix_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
